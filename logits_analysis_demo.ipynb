{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67df7d28",
   "metadata": {},
   "source": [
    "# Qwen架构LLM模型Logits研究分析\n",
    "\n",
    "这个notebook演示如何对Qwen架构的LLM模型进行logits分析，比较不同训练阶段模型的内部表征差异。\n",
    "\n",
    "## 研究目标\n",
    "- 分析三个模型（Baseline、SFT、RL）的logits差异\n",
    "- 观察推理关键词（'wait', 'aha', 'check'等）的logits变化\n",
    "- 多维度对比：模型间、难度级别间、正确性间的差异\n",
    "\n",
    "## 实验设置\n",
    "- 使用transformers库加载模型\n",
    "- 分析JSONL格式的query-answer数据\n",
    "- 生成热力图和折线图可视化结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff84af2f",
   "metadata": {},
   "source": [
    "# 1. 环境设置与依赖导入\n",
    "\n",
    "首先确保使用uv管理的虚拟环境，并导入所需的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 深度学习相关\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "# 设置随机种子以确保结果可重现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"所有依赖已成功导入！\")\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU数量: {torch.cuda.device_count()}\")\n",
    "    print(f\"当前GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b852aa",
   "metadata": {},
   "source": [
    "# 2. 模型加载与配置\n",
    "\n",
    "配置三个模型的路径和参数设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73846075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型配置\n",
    "MODEL_CONFIGS = {\n",
    "    'baseline': {\n",
    "        'name': 'Qwen2.5-Math-7B',\n",
    "        'path': '/gpfs/models/huggingface.co/Qwen/Qwen2.5-Math-7B',\n",
    "        'description': 'Baseline Qwen Model'\n",
    "    },\n",
    "    'sft': {\n",
    "        'name': 'DeepSeek-R1-Distill-Qwen-7B',\n",
    "        'path': '/gpfs/models/huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/',\n",
    "        'description': 'SFT Model'\n",
    "    },\n",
    "    'rl': {\n",
    "        'name': 'RL-DeepSeek-R1-Distill-Qwen-7B',\n",
    "        'path': '/gpfs/users/xizhiheng/qiji_projects/NorthRL/checkpoints/xizhiheng_SkyWork_runs_DeepSeek-R1-Distill-Qwen-7B-Rethink-RL-Loss/xizhiheng___fsdp_valFirst_lr2e-6_kl0-low0.4-high0.2-partial-budget-1-len32k-skywork-grpo-temperature0.6-ppo_epochs2-stale1-testtmp-4-testdynamicclip-targetpos0.5-left---right-1.0-inf-1/global_step_380',\n",
    "        'description': 'RL Trained Model'\n",
    "    }\n",
    "}\n",
    "\n",
    "# 分析配置\n",
    "TARGET_TOKENS = ['wait', 'aha', 'check', 'think', 'hmm', 'let', 'actually', 'however', 'so', 'therefore']\n",
    "MAX_LENGTH = 2048\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "print(\"模型配置已设置：\")\n",
    "for key, config in MODEL_CONFIGS.items():\n",
    "    print(f\"  {key}: {config['description']}\")\n",
    "print(f\"\\\\n目标分析词汇: {TARGET_TOKENS}\")\n",
    "print(f\"设备: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a74bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    \"\"\"模型加载器类\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, device: str = 'cuda'):\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"加载模型和tokenizer\"\"\"\n",
    "        print(f\"正在加载模型: {self.model_path}\")\n",
    "        \n",
    "        try:\n",
    "            # 加载tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_path, \n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # 设置pad_token\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # 加载模型\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            self.model.eval()\n",
    "            print(f\"模型加载成功！\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"模型加载失败: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def is_loaded(self):\n",
    "        \"\"\"检查模型是否已加载\"\"\"\n",
    "        return self.model is not None and self.tokenizer is not None\n",
    "\n",
    "# 创建模型加载器实例（暂不加载，避免内存占用）\n",
    "model_loaders = {}\n",
    "for model_name, config in MODEL_CONFIGS.items():\n",
    "    model_loaders[model_name] = ModelLoader(config['path'], DEVICE)\n",
    "\n",
    "print(\"模型加载器已创建。使用时调用 load_model() 方法加载模型。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b2752",
   "metadata": {},
   "source": [
    "# 3. 数据加载与预处理\n",
    "\n",
    "加载JSONL格式的数据，包含query、answer和level字段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d4f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"从JSONL文件加载数据\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with jsonlines.open(file_path) as reader:\n",
    "            data = list(reader)\n",
    "        print(f\"成功加载 {len(data)} 条数据\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"文件 {file_path} 不存在，将创建示例数据\")\n",
    "        data = create_sample_data()\n",
    "    return data\n",
    "\n",
    "def create_sample_data() -> List[Dict[str, Any]]:\n",
    "    \"\"\"创建示例数据用于演示\"\"\"\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"query\": \"What is the derivative of x^2 + 3x + 1?\",\n",
    "            \"answer\": \"Wait, let me think about this step by step. The derivative of x^2 is 2x, the derivative of 3x is 3, and the derivative of 1 is 0. So the answer is 2x + 3.\",\n",
    "            \"level\": 2,\n",
    "            \"is_correct\": True\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"Solve the equation 2x + 5 = 15\",\n",
    "            \"answer\": \"Hmm, I need to isolate x. So 2x = 15 - 5 = 10, therefore x = 5.\",\n",
    "            \"level\": 1,\n",
    "            \"is_correct\": True\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"Find the area of a circle with radius 5\",\n",
    "            \"answer\": \"Actually, the formula for area of a circle is πr². So it's π × 5² = 25π.\",\n",
    "            \"level\": 1,\n",
    "            \"is_correct\": True\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"What is the integral of sin(x)?\",\n",
    "            \"answer\": \"Aha! The integral of sin(x) is -cos(x) + C, where C is the constant of integration.\",\n",
    "            \"level\": 3,\n",
    "            \"is_correct\": True\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"Solve x^2 - 4x + 3 = 0\",\n",
    "            \"answer\": \"Let me check... using the quadratic formula or factoring: (x-1)(x-3) = 0, so x = 1 or x = 3.\",\n",
    "            \"level\": 2,\n",
    "            \"is_correct\": True\n",
    "        }\n",
    "    ]\n",
    "    return sample_data\n",
    "\n",
    "def preprocess_data(data: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"预处理数据\"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # 创建完整文本\n",
    "    df['full_text'] = df.apply(lambda row: f\"Query: {row['query']}\\\\nAnswer: {row['answer']}\", axis=1)\n",
    "    \n",
    "    # 计算文本长度\n",
    "    df['query_length'] = df['query'].str.len()\n",
    "    df['answer_length'] = df['answer'].str.len()\n",
    "    df['full_text_length'] = df['full_text'].str.len()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 加载数据\n",
    "data_file = \"data/queries.jsonl\"\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "data = load_data(data_file)\n",
    "df = preprocess_data(data)\n",
    "\n",
    "print(f\"数据预处理完成！\")\n",
    "print(f\"数据形状: {df.shape}\")\n",
    "print(f\"\\\\n数据概览:\")\n",
    "print(df[['level', 'query_length', 'answer_length', 'is_correct']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据分布可视化\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# 难度级别分布\n",
    "df['level'].value_counts().sort_index().plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "axes[0,0].set_title('难度级别分布')\n",
    "axes[0,0].set_xlabel('难度级别')\n",
    "axes[0,0].set_ylabel('数量')\n",
    "\n",
    "# 正确性分布\n",
    "if 'is_correct' in df.columns:\n",
    "    df['is_correct'].value_counts().plot(kind='bar', ax=axes[0,1], color='lightgreen')\n",
    "    axes[0,1].set_title('答题正确性分布')\n",
    "    axes[0,1].set_xlabel('是否正确')\n",
    "    axes[0,1].set_ylabel('数量')\n",
    "\n",
    "# 文本长度分布\n",
    "axes[1,0].hist(df['full_text_length'], bins=10, color='lightcoral', alpha=0.7)\n",
    "axes[1,0].set_title('文本长度分布')\n",
    "axes[1,0].set_xlabel('文本长度')\n",
    "axes[1,0].set_ylabel('频次')\n",
    "\n",
    "# 关键词出现频次\n",
    "keyword_counts = {token: 0 for token in TARGET_TOKENS}\n",
    "for text in df['full_text']:\n",
    "    text_lower = text.lower()\n",
    "    for token in TARGET_TOKENS:\n",
    "        keyword_counts[token] += text_lower.count(token)\n",
    "\n",
    "axes[1,1].bar(keyword_counts.keys(), keyword_counts.values(), color='gold')\n",
    "axes[1,1].set_title('关键词出现频次')\n",
    "axes[1,1].set_xlabel('关键词')\n",
    "axes[1,1].set_ylabel('出现次数')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\\\n关键词统计:\")\n",
    "for token, count in keyword_counts.items():\n",
    "    print(f\"  {token}: {count} 次\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122eae6",
   "metadata": {},
   "source": [
    "# 4. Logits提取函数实现\n",
    "\n",
    "实现从模型中提取每个token的logits的核心功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitsExtractor:\n",
    "    \"\"\"Logits提取器\"\"\"\n",
    "    \n",
    "    def __init__(self, model_loader: ModelLoader):\n",
    "        self.model_loader = model_loader\n",
    "    \n",
    "    def extract_logits(self, text: str, max_length: int = MAX_LENGTH) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        提取文本的logits\n",
    "        \n",
    "        Returns:\n",
    "            logits: shape (seq_len, vocab_size)\n",
    "            tokens: token列表\n",
    "        \"\"\"\n",
    "        if not self.model_loader.is_loaded():\n",
    "            raise ValueError(\"模型未加载，请先调用 load_model()\")\n",
    "        \n",
    "        # 编码输入\n",
    "        inputs = self.model_loader.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].to(self.model_loader.device)\n",
    "        attention_mask = inputs['attention_mask'].to(self.model_loader.device)\n",
    "        \n",
    "        # 获取tokens\n",
    "        tokens = self.model_loader.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        \n",
    "        # 提取logits\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model_loader.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits[0].cpu().numpy()  # (seq_len, vocab_size)\n",
    "        \n",
    "        return logits, tokens\n",
    "    \n",
    "    def get_token_probabilities(self, logits: np.ndarray, token_ids: List[int]) -> np.ndarray:\n",
    "        \"\"\"获取特定token的概率\"\"\"\n",
    "        # 应用softmax获取概率\n",
    "        probabilities = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "        \n",
    "        # 提取目标token的概率\n",
    "        target_probs = probabilities[:, token_ids]\n",
    "        \n",
    "        return target_probs\n",
    "    \n",
    "    def find_target_positions(self, tokens: List[str], target_tokens: List[str]) -> Dict[str, List[int]]:\n",
    "        \"\"\"找到目标token在序列中的位置\"\"\"\n",
    "        positions = {token: [] for token in target_tokens}\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            # 处理不同的tokenization格式\n",
    "            cleaned_token = token.replace('Ġ', '').replace('▁', '').lower().strip()\n",
    "            for target in target_tokens:\n",
    "                if cleaned_token == target.lower() or token.lower() == target.lower():\n",
    "                    positions[target].append(i)\n",
    "        \n",
    "        return positions\n",
    "\n",
    "def analyze_single_text(extractor: LogitsExtractor, text: str, target_tokens: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"分析单个文本的logits\"\"\"\n",
    "    # 提取logits\n",
    "    logits, tokens = extractor.extract_logits(text)\n",
    "    \n",
    "    # 找到目标token位置\n",
    "    positions = extractor.find_target_positions(tokens, target_tokens)\n",
    "    \n",
    "    # 获取目标token的ID\n",
    "    target_token_ids = []\n",
    "    for token in target_tokens:\n",
    "        try:\n",
    "            # 尝试不同的编码方式\n",
    "            variations = [token, token.capitalize(), f\" {token}\"]\n",
    "            for var in variations:\n",
    "                encoded = extractor.model_loader.tokenizer.encode(var, add_special_tokens=False)\n",
    "                if encoded:\n",
    "                    target_token_ids.append(encoded[0])\n",
    "                    break\n",
    "            else:\n",
    "                target_token_ids.append(0)  # 如果找不到，使用0\n",
    "        except:\n",
    "            target_token_ids.append(0)\n",
    "    \n",
    "    # 获取概率\n",
    "    probabilities = extractor.get_token_probabilities(logits, target_token_ids)\n",
    "    \n",
    "    return {\n",
    "        'logits': logits,\n",
    "        'tokens': tokens,\n",
    "        'positions': positions,\n",
    "        'probabilities': probabilities,\n",
    "        'target_token_ids': target_token_ids\n",
    "    }\n",
    "\n",
    "print(\"Logits提取器已定义完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a94cd66",
   "metadata": {},
   "source": [
    "# 5. 特定词汇位置检测\n",
    "\n",
    "实现检测关键推理词汇在文本中位置的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6780487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_context_around_tokens(logits: np.ndarray, positions: Dict[str, List[int]], \n",
    "                                 tokens: List[str], context_window: int = 5) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"分析目标token周围的上下文logits\"\"\"\n",
    "    context_analysis = {}\n",
    "    \n",
    "    for target_token, pos_list in positions.items():\n",
    "        if not pos_list:\n",
    "            continue\n",
    "            \n",
    "        token_analysis = {\n",
    "            'positions': pos_list,\n",
    "            'context_logits': [],\n",
    "            'context_tokens': [],\n",
    "            'logits_changes': []\n",
    "        }\n",
    "        \n",
    "        for pos in pos_list:\n",
    "            # 获取上下文窗口\n",
    "            start_idx = max(0, pos - context_window)\n",
    "            end_idx = min(len(tokens), pos + context_window + 1)\n",
    "            \n",
    "            # 提取上下文logits和tokens\n",
    "            context_logits = logits[start_idx:end_idx]\n",
    "            context_tokens = tokens[start_idx:end_idx]\n",
    "            \n",
    "            token_analysis['context_logits'].append(context_logits)\n",
    "            token_analysis['context_tokens'].append(context_tokens)\n",
    "            \n",
    "            # 分析logits变化（相对于前一个token）\n",
    "            if pos > 0:\n",
    "                current_logits = logits[pos]\n",
    "                previous_logits = logits[pos - 1]\n",
    "                logits_change = np.mean(current_logits - previous_logits)\n",
    "                token_analysis['logits_changes'].append(logits_change)\n",
    "        \n",
    "        context_analysis[target_token] = token_analysis\n",
    "    \n",
    "    return context_analysis\n",
    "\n",
    "def visualize_token_positions(tokens: List[str], positions: Dict[str, List[int]], \n",
    "                            title: str = \"Token Positions\") -> None:\n",
    "    \"\"\"可视化token位置\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 4))\n",
    "    \n",
    "    # 绘制所有token\n",
    "    x_positions = range(len(tokens))\n",
    "    ax.scatter(x_positions, [0] * len(tokens), c='lightgray', s=20, alpha=0.5)\n",
    "    \n",
    "    # 突出显示目标token\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(TARGET_TOKENS)))\n",
    "    \n",
    "    for i, (target_token, pos_list) in enumerate(positions.items()):\n",
    "        if pos_list:\n",
    "            ax.scatter(pos_list, [0] * len(pos_list), \n",
    "                      c=[colors[i]], s=100, label=target_token, alpha=0.8)\n",
    "            \n",
    "            # 添加标签\n",
    "            for pos in pos_list:\n",
    "                ax.annotate(target_token, (pos, 0), xytext=(0, 20), \n",
    "                           textcoords='offset points', ha='center',\n",
    "                           bbox=dict(boxstyle='round,pad=0.3', facecolor=colors[i], alpha=0.7))\n",
    "    \n",
    "    ax.set_xlabel('Token Position')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 显示部分token文本（避免过于拥挤）\n",
    "    step = max(1, len(tokens) // 20)  # 显示大约20个token\n",
    "    ax.set_xticks(range(0, len(tokens), step))\n",
    "    ax.set_xticklabels([tokens[i][:10] + '...' if len(tokens[i]) > 10 else tokens[i] \n",
    "                       for i in range(0, len(tokens), step)], rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 演示位置检测功能\n",
    "sample_text = df.iloc[0]['full_text']\n",
    "print(f\"示例文本: {sample_text[:200]}...\")\n",
    "print(\"\\\\n准备分析token位置...\")\n",
    "\n",
    "# 注意：这里我们使用一个简化的tokenizer来演示概念\n",
    "# 实际使用时需要加载真实的模型\n",
    "def demo_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"简化的tokenizer用于演示\"\"\"\n",
    "    import re\n",
    "    # 简单的单词分割\n",
    "    tokens = re.findall(r\"\\\\b\\\\w+\\\\b|[.,!?;]\", text.lower())\n",
    "    return tokens\n",
    "\n",
    "demo_tokens = demo_tokenize(sample_text)\n",
    "demo_positions = {}\n",
    "for token in TARGET_TOKENS:\n",
    "    demo_positions[token] = [i for i, t in enumerate(demo_tokens) if t == token]\n",
    "\n",
    "print(f\"发现的目标token位置:\")\n",
    "for token, positions in demo_positions.items():\n",
    "    if positions:\n",
    "        print(f\"  {token}: 位置 {positions}\")\n",
    "\n",
    "# 可视化演示\n",
    "visualize_token_positions(demo_tokens, demo_positions, \"演示：目标Token位置\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea1a094",
   "metadata": {},
   "source": [
    "# 6. 单模型Logits分析\n",
    "\n",
    "在实际环境中加载单个模型并分析其logits模式。\n",
    "\n",
    "**注意**: 由于模型较大，这里提供了完整的代码框架。在实际运行时请确保有足够的GPU内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e3597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单模型分析函数\n",
    "def analyze_single_model(model_name: str, texts: List[str], target_tokens: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"分析单个模型的logits\"\"\"\n",
    "    \n",
    "    print(f\"开始分析模型: {model_name}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    model_loader = model_loaders[model_name]\n",
    "    if not model_loader.load_model():\n",
    "        print(f\"模型 {model_name} 加载失败\")\n",
    "        return None\n",
    "    \n",
    "    extractor = LogitsExtractor(model_loader)\n",
    "    \n",
    "    model_results = {\n",
    "        'model_name': model_name,\n",
    "        'text_analyses': [],\n",
    "        'summary_stats': {}\n",
    "    }\n",
    "    \n",
    "    all_positions = {token: [] for token in target_tokens}\n",
    "    all_probabilities = {token: [] for token in target_tokens}\n",
    "    \n",
    "    # 分析每个文本\n",
    "    for i, text in enumerate(tqdm(texts, desc=f\"分析 {model_name}\")):\n",
    "        try:\n",
    "            result = analyze_single_text(extractor, text, target_tokens)\n",
    "            result['text_index'] = i\n",
    "            model_results['text_analyses'].append(result)\n",
    "            \n",
    "            # 收集统计信息\n",
    "            for token, positions in result['positions'].items():\n",
    "                all_positions[token].extend(positions)\n",
    "                \n",
    "                # 如果找到了token，记录其概率\n",
    "                if positions and len(result['probabilities']) > 0:\n",
    "                    token_idx = target_tokens.index(token)\n",
    "                    for pos in positions:\n",
    "                        if pos < len(result['probabilities']):\n",
    "                            all_probabilities[token].append(result['probabilities'][pos, token_idx])\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"分析文本 {i} 时出错: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 计算汇总统计\n",
    "    model_results['summary_stats'] = {\n",
    "        'total_texts': len(texts),\n",
    "        'processed_texts': len(model_results['text_analyses']),\n",
    "        'token_frequencies': {token: len(positions) for token, positions in all_positions.items()},\n",
    "        'average_probabilities': {token: np.mean(probs) if probs else 0.0 \n",
    "                                for token, probs in all_probabilities.items()},\n",
    "        'std_probabilities': {token: np.std(probs) if probs else 0.0 \n",
    "                            for token, probs in all_probabilities.items()}\n",
    "    }\n",
    "    \n",
    "    print(f\"模型 {model_name} 分析完成\")\n",
    "    return model_results\n",
    "\n",
    "# 创建模拟分析结果（用于演示，因为实际模型可能无法在此环境中加载）\n",
    "def create_mock_analysis_results() -> Dict[str, Any]:\n",
    "    \"\"\"创建模拟的分析结果用于演示可视化\"\"\"\n",
    "    \n",
    "    mock_results = {}\n",
    "    \n",
    "    for model_name in ['baseline', 'sft', 'rl']:\n",
    "        # 模拟不同模型的概率分布\n",
    "        base_probs = {\n",
    "            'wait': 0.1, 'aha': 0.05, 'check': 0.08, 'think': 0.12, 'hmm': 0.03,\n",
    "            'let': 0.15, 'actually': 0.07, 'however': 0.06, 'so': 0.20, 'therefore': 0.09\n",
    "        }\n",
    "        \n",
    "        # 为不同模型添加一些变化\n",
    "        if model_name == 'sft':\n",
    "            # SFT模型可能在推理词汇上有更高概率\n",
    "            for token in ['think', 'check', 'therefore']:\n",
    "                base_probs[token] *= 1.3\n",
    "        elif model_name == 'rl':\n",
    "            # RL模型可能在确定性词汇上概率更高\n",
    "            for token in ['aha', 'actually', 'so']:\n",
    "                base_probs[token] *= 1.5\n",
    "        \n",
    "        mock_results[model_name] = {\n",
    "            'model_name': model_name,\n",
    "            'summary_stats': {\n",
    "                'total_texts': len(df),\n",
    "                'processed_texts': len(df),\n",
    "                'token_frequencies': {token: np.random.randint(1, 10) for token in TARGET_TOKENS},\n",
    "                'average_probabilities': base_probs,\n",
    "                'std_probabilities': {token: prob * 0.3 for token, prob in base_probs.items()}\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return mock_results\n",
    "\n",
    "# 演示：创建模拟结果\n",
    "print(\"创建模拟分析结果用于演示...\")\n",
    "mock_results = create_mock_analysis_results()\n",
    "\n",
    "# 显示结果概览\n",
    "for model_name, results in mock_results.items():\n",
    "    print(f\"\\\\n{model_name.upper()} 模型统计:\")\n",
    "    stats = results['summary_stats']\n",
    "    print(f\"  处理文本数: {stats['processed_texts']}\")\n",
    "    print(f\"  目标token平均概率:\")\n",
    "    for token, prob in stats['average_probabilities'].items():\n",
    "        print(f\"    {token}: {prob:.3f} ± {stats['std_probabilities'][token]:.3f}\")\n",
    "\n",
    "print(\"\\\\n如需分析真实模型，请取消注释以下代码：\")\n",
    "print(\"# real_results = {}\")\n",
    "print(\"# for model_name in ['baseline']:  # 先从一个模型开始\")\n",
    "print(\"#     texts = df['full_text'].tolist()\")\n",
    "print(\"#     real_results[model_name] = analyze_single_model(model_name, texts, TARGET_TOKENS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45225ed9",
   "metadata": {},
   "source": [
    "# 7. 多模型对比分析\n",
    "\n",
    "比较Baseline、SFT和RL三个模型的logits差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237564c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(results_dict: Dict[str, Dict], metric: str = 'average_probabilities') -> pd.DataFrame:\n",
    "    \"\"\"比较多个模型的结果\"\"\"\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, results in results_dict.items():\n",
    "        stats = results['summary_stats']\n",
    "        for token, value in stats[metric].items():\n",
    "            comparison_data.append({\n",
    "                'model': model_name,\n",
    "                'token': token,\n",
    "                'value': value\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "def plot_model_comparison(results_dict: Dict[str, Dict]):\n",
    "    \"\"\"绘制模型对比图\"\"\"\n",
    "    \n",
    "    # 准备数据\n",
    "    df_comparison = compare_models(results_dict, 'average_probabilities')\n",
    "    \n",
    "    # 创建子图\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. 分组柱状图\n",
    "    pivot_data = df_comparison.pivot(index='token', columns='model', values='value')\n",
    "    pivot_data.plot(kind='bar', ax=axes[0, 0], width=0.8)\n",
    "    axes[0, 0].set_title('模型间目标Token平均概率对比')\n",
    "    axes[0, 0].set_ylabel('平均概率')\n",
    "    axes[0, 0].legend(title='模型')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. 热力图\n",
    "    heatmap_data = pivot_data.T\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('模型-Token概率热力图')\n",
    "    \n",
    "    # 3. 雷达图（使用极坐标）\n",
    "    angles = np.linspace(0, 2 * np.pi, len(TARGET_TOKENS), endpoint=False)\n",
    "    angles = np.concatenate((angles, [angles[0]]))  # 闭合图形\n",
    "    \n",
    "    ax_radar = plt.subplot(2, 2, 3, projection='polar')\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    \n",
    "    for i, (model_name, results) in enumerate(results_dict.items()):\n",
    "        values = [results['summary_stats']['average_probabilities'][token] for token in TARGET_TOKENS]\n",
    "        values += [values[0]]  # 闭合图形\n",
    "        \n",
    "        ax_radar.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i])\n",
    "        ax_radar.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    ax_radar.set_xticks(angles[:-1])\n",
    "    ax_radar.set_xticklabels(TARGET_TOKENS)\n",
    "    ax_radar.set_title('模型性能雷达图')\n",
    "    ax_radar.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    \n",
    "    # 4. 箱线图（模拟标准差）\n",
    "    box_data = []\n",
    "    box_labels = []\n",
    "    for model_name, results in results_dict.items():\n",
    "        for token in TARGET_TOKENS:\n",
    "            mean_prob = results['summary_stats']['average_probabilities'][token]\n",
    "            std_prob = results['summary_stats']['std_probabilities'][token]\n",
    "            # 模拟数据点\n",
    "            simulated_data = np.random.normal(mean_prob, std_prob, 50)\n",
    "            box_data.extend(simulated_data)\n",
    "            box_labels.extend([f\"{model_name}\\\\n{token}\"] * 50)\n",
    "    \n",
    "    df_box = pd.DataFrame({'value': box_data, 'model_token': box_labels})\n",
    "    \n",
    "    # 选择部分数据避免过于拥挤\n",
    "    selected_tokens = ['wait', 'think', 'check', 'so']\n",
    "    df_box_filtered = df_box[df_box['model_token'].str.contains('|'.join(selected_tokens))]\n",
    "    \n",
    "    sns.boxplot(data=df_box_filtered, x='model_token', y='value', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('概率分布箱线图（选择部分Token）')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 统计显著性测试\n",
    "def statistical_comparison(results_dict: Dict[str, Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"进行统计显著性测试\"\"\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    comparison_results = {}\n",
    "    models = list(results_dict.keys())\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        for j in range(i + 1, len(models)):\n",
    "            model1, model2 = models[i], models[j]\n",
    "            \n",
    "            # 收集两个模型的概率数据\n",
    "            probs1 = list(results_dict[model1]['summary_stats']['average_probabilities'].values())\n",
    "            probs2 = list(results_dict[model2]['summary_stats']['average_probabilities'].values())\n",
    "            \n",
    "            # 进行t检验\n",
    "            t_stat, p_value = stats.ttest_ind(probs1, probs2)\n",
    "            \n",
    "            comparison_results[f\"{model1}_vs_{model2}\"] = {\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05\n",
    "            }\n",
    "    \n",
    "    return comparison_results\n",
    "\n",
    "# 执行模型对比分析\n",
    "print(\"=== 模型对比分析 ===\")\n",
    "\n",
    "# 绘制对比图\n",
    "plot_model_comparison(mock_results)\n",
    "\n",
    "# 统计测试\n",
    "stats_results = statistical_comparison(mock_results)\n",
    "print(\"\\\\n统计显著性测试结果:\")\n",
    "for comparison, result in stats_results.items():\n",
    "    models = comparison.replace('_vs_', ' vs ')\n",
    "    significance = \"显著\" if result['significant'] else \"不显著\"\n",
    "    print(f\"  {models}: t={result['t_statistic']:.3f}, p={result['p_value']:.3f} ({significance})\")\n",
    "\n",
    "# 模型差异分析\n",
    "print(\"\\\\n=== 模型间差异分析 ===\")\n",
    "for i, model1 in enumerate(['baseline', 'sft', 'rl']):\n",
    "    for j, model2 in enumerate(['baseline', 'sft', 'rl']):\n",
    "        if i < j:\n",
    "            print(f\"\\\\n{model1.upper()} vs {model2.upper()}:\")\n",
    "            for token in TARGET_TOKENS:\n",
    "                prob1 = mock_results[model1]['summary_stats']['average_probabilities'][token]\n",
    "                prob2 = mock_results[model2]['summary_stats']['average_probabilities'][token]\n",
    "                diff = prob2 - prob1\n",
    "                change = \"增加\" if diff > 0 else \"减少\"\n",
    "                print(f\"  {token}: {change} {abs(diff):.3f} ({diff/prob1*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbc894",
   "metadata": {},
   "source": [
    "# 8. 难度级别对比分析\n",
    "\n",
    "分析不同难度级别(1-5)题目的logits模式差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ccaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_by_difficulty_level(df: pd.DataFrame, target_tokens: List[str]) -> Dict[int, Dict[str, float]]:\n",
    "    \"\"\"按难度级别分析token使用模式\"\"\"\n",
    "    \n",
    "    level_analysis = {}\n",
    "    \n",
    "    for level in range(1, 6):\n",
    "        level_data = df[df['level'] == level]\n",
    "        if len(level_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        level_token_stats = {}\n",
    "        \n",
    "        for token in target_tokens:\n",
    "            # 计算该级别中包含此token的文本比例\n",
    "            texts_with_token = level_data['full_text'].str.lower().str.contains(token, regex=False)\n",
    "            frequency = texts_with_token.sum() / len(level_data)\n",
    "            \n",
    "            # 模拟概率数据（实际中应从logits分析获得）\n",
    "            base_prob = np.random.uniform(0.02, 0.25)\n",
    "            # 难度越高，某些推理词汇概率可能更高\n",
    "            if token in ['think', 'check', 'however', 'therefore'] and level > 3:\n",
    "                level_multiplier = 1 + (level - 3) * 0.2\n",
    "            else:\n",
    "                level_multiplier = 1.0\n",
    "            \n",
    "            level_token_stats[token] = {\n",
    "                'frequency': frequency,\n",
    "                'avg_probability': base_prob * level_multiplier,\n",
    "                'text_count': len(level_data)\n",
    "            }\n",
    "        \n",
    "        level_analysis[level] = level_token_stats\n",
    "    \n",
    "    return level_analysis\n",
    "\n",
    "def plot_difficulty_analysis(level_analysis: Dict[int, Dict[str, Dict[str, float]]]):\n",
    "    \"\"\"绘制难度级别分析图\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. 频率热力图\n",
    "    freq_data = []\n",
    "    prob_data = []\n",
    "    levels = sorted(level_analysis.keys())\n",
    "    \n",
    "    for level in levels:\n",
    "        freq_row = []\n",
    "        prob_row = []\n",
    "        for token in TARGET_TOKENS:\n",
    "            if token in level_analysis[level]:\n",
    "                freq_row.append(level_analysis[level][token]['frequency'])\n",
    "                prob_row.append(level_analysis[level][token]['avg_probability'])\n",
    "            else:\n",
    "                freq_row.append(0)\n",
    "                prob_row.append(0)\n",
    "        freq_data.append(freq_row)\n",
    "        prob_data.append(prob_row)\n",
    "    \n",
    "    # 频率热力图\n",
    "    sns.heatmap(freq_data, \n",
    "                xticklabels=TARGET_TOKENS, \n",
    "                yticklabels=[f'Level {l}' for l in levels],\n",
    "                annot=True, fmt='.2f', cmap='Blues', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Token出现频率按难度级别')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 概率热力图\n",
    "    sns.heatmap(prob_data, \n",
    "                xticklabels=TARGET_TOKENS, \n",
    "                yticklabels=[f'Level {l}' for l in levels],\n",
    "                annot=True, fmt='.3f', cmap='Reds', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Token平均概率按难度级别')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. 折线图 - 选择几个关键token\n",
    "    key_tokens = ['think', 'check', 'wait', 'so']\n",
    "    for token in key_tokens:\n",
    "        frequencies = [level_analysis[level][token]['frequency'] \n",
    "                      if token in level_analysis[level] else 0 \n",
    "                      for level in levels]\n",
    "        axes[1, 0].plot(levels, frequencies, marker='o', label=token, linewidth=2)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('难度级别')\n",
    "    axes[1, 0].set_ylabel('出现频率')\n",
    "    axes[1, 0].set_title('关键Token频率变化趋势')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. 箱线图 - 各级别的总体token使用\n",
    "    box_data = []\n",
    "    box_labels = []\n",
    "    \n",
    "    for level in levels:\n",
    "        level_probs = [level_analysis[level][token]['avg_probability'] \n",
    "                      for token in TARGET_TOKENS \n",
    "                      if token in level_analysis[level]]\n",
    "        box_data.extend(level_probs)\n",
    "        box_labels.extend([f'Level {level}'] * len(level_probs))\n",
    "    \n",
    "    df_box = pd.DataFrame({'probability': box_data, 'level': box_labels})\n",
    "    sns.boxplot(data=df_box, x='level', y='probability', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('各难度级别Token概率分布')\n",
    "    axes[1, 1].set_ylabel('概率')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_difficulty_patterns(level_analysis: Dict[int, Dict[str, Dict[str, float]]]):\n",
    "    \"\"\"分析难度级别的模式\"\"\"\n",
    "    \n",
    "    print(\"=== 难度级别分析结果 ===\")\n",
    "    \n",
    "    # 计算相关性\n",
    "    levels = sorted(level_analysis.keys())\n",
    "    \n",
    "    for token in TARGET_TOKENS:\n",
    "        frequencies = []\n",
    "        probabilities = []\n",
    "        \n",
    "        for level in levels:\n",
    "            if token in level_analysis[level]:\n",
    "                frequencies.append(level_analysis[level][token]['frequency'])\n",
    "                probabilities.append(level_analysis[level][token]['avg_probability'])\n",
    "            else:\n",
    "                frequencies.append(0)\n",
    "                probabilities.append(0)\n",
    "        \n",
    "        # 计算与难度级别的相关性\n",
    "        freq_corr = np.corrcoef(levels, frequencies)[0, 1]\n",
    "        prob_corr = np.corrcoef(levels, probabilities)[0, 1]\n",
    "        \n",
    "        print(f\"\\\\n{token}:\")\n",
    "        print(f\"  频率与难度相关性: {freq_corr:.3f}\")\n",
    "        print(f\"  概率与难度相关性: {prob_corr:.3f}\")\n",
    "        \n",
    "        if abs(freq_corr) > 0.5:\n",
    "            trend = \"正相关\" if freq_corr > 0 else \"负相关\"\n",
    "            print(f\"  -> 频率与难度呈{trend} (|r|>0.5)\")\n",
    "        \n",
    "        if abs(prob_corr) > 0.5:\n",
    "            trend = \"正相关\" if prob_corr > 0 else \"负相关\"\n",
    "            print(f\"  -> 概率与难度呈{trend} (|r|>0.5)\")\n",
    "\n",
    "# 执行难度级别分析\n",
    "print(\"执行难度级别分析...\")\n",
    "level_analysis = analyze_by_difficulty_level(df, TARGET_TOKENS)\n",
    "\n",
    "# 绘制分析图\n",
    "plot_difficulty_analysis(level_analysis)\n",
    "\n",
    "# 分析模式\n",
    "analyze_difficulty_patterns(level_analysis)\n",
    "\n",
    "# 显示数据概览\n",
    "print(\"\\\\n=== 各级别数据概览 ===\")\n",
    "for level, stats in level_analysis.items():\n",
    "    text_count = next(iter(stats.values()))['text_count']\n",
    "    avg_freq = np.mean([data['frequency'] for data in stats.values()])\n",
    "    avg_prob = np.mean([data['avg_probability'] for data in stats.values()])\n",
    "    print(f\"Level {level}: {text_count} 条文本, 平均token频率: {avg_freq:.3f}, 平均概率: {avg_prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fff5a",
   "metadata": {},
   "source": [
    "# 9. 正确性对比分析\n",
    "\n",
    "分析答对和答错题目的logits差异，探索模型自信度与准确性的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_by_correctness(df: pd.DataFrame, target_tokens: List[str]) -> Dict[bool, Dict[str, float]]:\n",
    "    \"\"\"按答题正确性分析token模式\"\"\"\n",
    "    \n",
    "    correctness_analysis = {}\n",
    "    \n",
    "    for is_correct in [True, False]:\n",
    "        subset = df[df['is_correct'] == is_correct]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        \n",
    "        token_stats = {}\n",
    "        \n",
    "        for token in target_tokens:\n",
    "            # 计算token出现频率\n",
    "            texts_with_token = subset['full_text'].str.lower().str.contains(token, regex=False)\n",
    "            frequency = texts_with_token.sum() / len(subset)\n",
    "            \n",
    "            # 模拟概率数据\n",
    "            base_prob = np.random.uniform(0.03, 0.20)\n",
    "            \n",
    "            # 假设：正确答案中某些推理词汇概率更高\n",
    "            if is_correct and token in ['check', 'think', 'therefore', 'actually']:\n",
    "                correctness_multiplier = 1.3\n",
    "            elif not is_correct and token in ['wait', 'hmm', 'aha']:\n",
    "                correctness_multiplier = 1.2  # 错误答案中犹豫词汇更多\n",
    "            else:\n",
    "                correctness_multiplier = 1.0\n",
    "            \n",
    "            token_stats[token] = {\n",
    "                'frequency': frequency,\n",
    "                'avg_probability': base_prob * correctness_multiplier,\n",
    "                'text_count': len(subset),\n",
    "                'variance': (base_prob * correctness_multiplier) * 0.3\n",
    "            }\n",
    "        \n",
    "        correctness_analysis[is_correct] = token_stats\n",
    "    \n",
    "    return correctness_analysis\n",
    "\n",
    "def plot_correctness_analysis(correctness_analysis: Dict[bool, Dict[str, Dict[str, float]]]):\n",
    "    \"\"\"绘制正确性分析图\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 准备数据\n",
    "    correct_data = correctness_analysis[True]\n",
    "    incorrect_data = correctness_analysis[False]\n",
    "    \n",
    "    # 1. 对比柱状图 - 频率\n",
    "    tokens = TARGET_TOKENS\n",
    "    correct_freq = [correct_data[token]['frequency'] for token in tokens]\n",
    "    incorrect_freq = [incorrect_data[token]['frequency'] for token in tokens]\n",
    "    \n",
    "    x = np.arange(len(tokens))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 0].bar(x - width/2, correct_freq, width, label='正确答案', color='green', alpha=0.7)\n",
    "    axes[0, 0].bar(x + width/2, incorrect_freq, width, label='错误答案', color='red', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Token')\n",
    "    axes[0, 0].set_ylabel('出现频率')\n",
    "    axes[0, 0].set_title('Token出现频率：正确 vs 错误答案')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(tokens, rotation=45)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 对比柱状图 - 概率\n",
    "    correct_prob = [correct_data[token]['avg_probability'] for token in tokens]\n",
    "    incorrect_prob = [incorrect_data[token]['avg_probability'] for token in tokens]\n",
    "    \n",
    "    axes[0, 1].bar(x - width/2, correct_prob, width, label='正确答案', color='green', alpha=0.7)\n",
    "    axes[0, 1].bar(x + width/2, incorrect_prob, width, label='错误答案', color='red', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Token')\n",
    "    axes[0, 1].set_ylabel('平均概率')\n",
    "    axes[0, 1].set_title('Token平均概率：正确 vs 错误答案')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(tokens, rotation=45)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. 差异热力图\n",
    "    freq_diff = [correct_freq[i] - incorrect_freq[i] for i in range(len(tokens))]\n",
    "    prob_diff = [correct_prob[i] - incorrect_prob[i] for i in range(len(tokens))]\n",
    "    \n",
    "    diff_data = np.array([freq_diff, prob_diff])\n",
    "    \n",
    "    sns.heatmap(diff_data, \n",
    "                xticklabels=tokens, \n",
    "                yticklabels=['频率差异', '概率差异'],\n",
    "                annot=True, fmt='.3f', cmap='RdBu_r', center=0, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('正确 - 错误答案的差异\\\\n(正值表示正确答案中更高)')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. 散点图 - 频率vs概率\n",
    "    for is_correct, label, color in [(True, '正确答案', 'green'), (False, '错误答案', 'red')]:\n",
    "        data = correctness_analysis[is_correct]\n",
    "        freqs = [data[token]['frequency'] for token in tokens]\n",
    "        probs = [data[token]['avg_probability'] for token in tokens]\n",
    "        \n",
    "        axes[1, 1].scatter(freqs, probs, label=label, color=color, alpha=0.7, s=60)\n",
    "        \n",
    "        # 添加token标签\n",
    "        for i, token in enumerate(tokens):\n",
    "            axes[1, 1].annotate(token, (freqs[i], probs[i]), \n",
    "                              xytext=(3, 3), textcoords='offset points', \n",
    "                              fontsize=8, alpha=0.8)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('出现频率')\n",
    "    axes[1, 1].set_ylabel('平均概率')\n",
    "    axes[1, 1].set_title('Token频率 vs 概率分布')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def correctness_statistical_analysis(correctness_analysis: Dict[bool, Dict[str, Dict[str, float]]]):\n",
    "    \"\"\"正确性统计分析\"\"\"\n",
    "    \n",
    "    print(\"=== 正确性统计分析 ===\")\n",
    "    \n",
    "    correct_data = correctness_analysis[True]\n",
    "    incorrect_data = correctness_analysis[False]\n",
    "    \n",
    "    print(f\"正确答案数量: {correct_data[TARGET_TOKENS[0]]['text_count']}\")\n",
    "    print(f\"错误答案数量: {incorrect_data[TARGET_TOKENS[0]]['text_count']}\")\n",
    "    \n",
    "    # 计算显著差异的token\n",
    "    significant_tokens = []\n",
    "    \n",
    "    print(\"\\\\n各Token在正确性上的差异:\")\n",
    "    for token in TARGET_TOKENS:\n",
    "        correct_freq = correct_data[token]['frequency']\n",
    "        incorrect_freq = incorrect_data[token]['frequency']\n",
    "        correct_prob = correct_data[token]['avg_probability']\n",
    "        incorrect_prob = incorrect_data[token]['avg_probability']\n",
    "        \n",
    "        freq_diff = correct_freq - incorrect_freq\n",
    "        prob_diff = correct_prob - incorrect_prob\n",
    "        \n",
    "        # 简单的差异判断（实际应用中可以用统计检验）\n",
    "        is_significant = abs(freq_diff) > 0.1 or abs(prob_diff) > 0.05\n",
    "        \n",
    "        print(f\"\\\\n{token}:\")\n",
    "        print(f\"  频率差异: {freq_diff:+.3f} ({'显著' if abs(freq_diff) > 0.1 else '不显著'})\")\n",
    "        print(f\"  概率差异: {prob_diff:+.3f} ({'显著' if abs(prob_diff) > 0.05 else '不显著'})\")\n",
    "        \n",
    "        if is_significant:\n",
    "            significant_tokens.append(token)\n",
    "            if freq_diff > 0:\n",
    "                print(f\"  -> {token} 在正确答案中更常见\")\n",
    "            else:\n",
    "                print(f\"  -> {token} 在错误答案中更常见\")\n",
    "    \n",
    "    print(f\"\\\\n显著差异的Token: {significant_tokens}\")\n",
    "    \n",
    "    # 计算整体模式\n",
    "    correct_avg_freq = np.mean([correct_data[token]['frequency'] for token in TARGET_TOKENS])\n",
    "    incorrect_avg_freq = np.mean([incorrect_data[token]['frequency'] for token in TARGET_TOKENS])\n",
    "    correct_avg_prob = np.mean([correct_data[token]['avg_probability'] for token in TARGET_TOKENS])\n",
    "    incorrect_avg_prob = np.mean([incorrect_data[token]['avg_probability'] for token in TARGET_TOKENS])\n",
    "    \n",
    "    print(f\"\\\\n整体模式:\")\n",
    "    print(f\"  正确答案平均token频率: {correct_avg_freq:.3f}\")\n",
    "    print(f\"  错误答案平均token频率: {incorrect_avg_freq:.3f}\")\n",
    "    print(f\"  正确答案平均token概率: {correct_avg_prob:.3f}\")\n",
    "    print(f\"  错误答案平均token概率: {incorrect_avg_prob:.3f}\")\n",
    "\n",
    "# 执行正确性分析\n",
    "print(\"执行正确性对比分析...\")\n",
    "correctness_analysis = analyze_by_correctness(df, TARGET_TOKENS)\n",
    "\n",
    "# 绘制分析图\n",
    "plot_correctness_analysis(correctness_analysis)\n",
    "\n",
    "# 统计分析\n",
    "correctness_statistical_analysis(correctness_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee7262e",
   "metadata": {},
   "source": [
    "# 10. 可视化热力图实现\n",
    "\n",
    "创建高级的logits热力图，突出显示关键词位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec0e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_heatmap(logits_data: Dict[str, np.ndarray], \n",
    "                          tokens: List[str], \n",
    "                          target_positions: Dict[str, List[int]],\n",
    "                          title: str = \"Logits Heatmap\") -> None:\n",
    "    \"\"\"创建高级logits热力图\"\"\"\n",
    "    \n",
    "    # 模拟logits数据（实际中从模型获取）\n",
    "    seq_len = len(tokens)\n",
    "    vocab_size = 1000  # 假设词汇表大小\n",
    "    \n",
    "    # 创建模拟的logits矩阵\n",
    "    simulated_logits = {}\n",
    "    for model_name in logits_data.keys():\n",
    "        # 创建基础logits矩阵\n",
    "        base_logits = np.random.randn(seq_len, vocab_size) * 2\n",
    "        \n",
    "        # 在目标token位置添加模式\n",
    "        for target_token, positions in target_positions.items():\n",
    "            for pos in positions:\n",
    "                if pos < seq_len:\n",
    "                    # 在目标位置附近增强某些维度\n",
    "                    enhancement_dims = np.random.choice(vocab_size, 50, replace=False)\n",
    "                    base_logits[pos, enhancement_dims] += np.random.uniform(2, 5, 50)\n",
    "        \n",
    "        simulated_logits[model_name] = base_logits\n",
    "    \n",
    "    # 创建多模型对比热力图\n",
    "    n_models = len(simulated_logits)\n",
    "    fig, axes = plt.subplots(n_models, 1, figsize=(20, 6 * n_models))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (model_name, logits) in enumerate(simulated_logits.items()):\n",
    "        # 选择top-k最重要的词汇维度进行可视化\n",
    "        top_k = 100\n",
    "        mean_logits = np.mean(logits, axis=0)\n",
    "        top_indices = np.argsort(mean_logits)[-top_k:]\n",
    "        \n",
    "        # 提取top-k维度的logits\n",
    "        heatmap_data = logits[:, top_indices].T\n",
    "        \n",
    "        # 创建热力图\n",
    "        im = axes[i].imshow(heatmap_data, cmap='viridis', aspect='auto', interpolation='nearest')\n",
    "        \n",
    "        # 标记目标token位置\n",
    "        for target_token, positions in target_positions.items():\n",
    "            for pos in positions:\n",
    "                if pos < seq_len:\n",
    "                    # 绘制垂直线标记\n",
    "                    axes[i].axvline(x=pos, color='red', linestyle='--', linewidth=2, alpha=0.8)\n",
    "                    \n",
    "                    # 添加token标签\n",
    "                    axes[i].text(pos, top_k // 2, target_token, \n",
    "                               rotation=90, verticalalignment='center',\n",
    "                               color='white', fontweight='bold', fontsize=10,\n",
    "                               bbox=dict(boxstyle='round,pad=0.2', facecolor='red', alpha=0.7))\n",
    "        \n",
    "        # 设置标签和标题\n",
    "        axes[i].set_title(f'{title} - {model_name.upper()}', fontsize=14, fontweight='bold')\n",
    "        axes[i].set_xlabel('Token Position')\n",
    "        axes[i].set_ylabel(f'Top {top_k} Vocabulary Dimensions')\n",
    "        \n",
    "        # 设置x轴标签（显示部分token）\n",
    "        step = max(1, seq_len // 15)\n",
    "        tick_positions = range(0, seq_len, step)\n",
    "        tick_labels = [tokens[i][:8] + '...' if len(tokens[i]) > 8 else tokens[i] \n",
    "                      for i in tick_positions]\n",
    "        axes[i].set_xticks(tick_positions)\n",
    "        axes[i].set_xticklabels(tick_labels, rotation=45, ha='right')\n",
    "        \n",
    "        # 添加颜色条\n",
    "        cbar = plt.colorbar(im, ax=axes[i], shrink=0.8)\n",
    "        cbar.set_label('Logits Value', rotation=270, labelpad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_interactive_heatmap(model_results: Dict[str, Dict], target_tokens: List[str]) -> go.Figure:\n",
    "    \"\"\"创建交互式热力图\"\"\"\n",
    "    \n",
    "    # 创建子图\n",
    "    n_models = len(model_results)\n",
    "    fig = make_subplots(\n",
    "        rows=n_models, cols=1,\n",
    "        subplot_titles=[f\"{name.upper()} Model\" for name in model_results.keys()],\n",
    "        vertical_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # 为每个模型创建热力图\n",
    "    for i, (model_name, results) in enumerate(model_results.items()):\n",
    "        # 模拟概率矩阵数据\n",
    "        seq_length = 50  # 假设序列长度\n",
    "        prob_matrix = np.random.rand(len(target_tokens), seq_length)\n",
    "        \n",
    "        # 在某些位置增强概率以模拟目标token\n",
    "        for j, token in enumerate(target_tokens):\n",
    "            # 随机选择几个位置作为该token的出现位置\n",
    "            positions = np.random.choice(seq_length, np.random.randint(1, 4), replace=False)\n",
    "            for pos in positions:\n",
    "                prob_matrix[j, pos] = np.random.uniform(0.7, 1.0)\n",
    "        \n",
    "        # 添加热力图\n",
    "        heatmap = go.Heatmap(\n",
    "            z=prob_matrix,\n",
    "            x=list(range(seq_length)),\n",
    "            y=target_tokens,\n",
    "            colorscale='Viridis',\n",
    "            showscale=(i == 0),  # 只在第一个子图显示色标\n",
    "            hoverongaps=False,\n",
    "            hovertemplate='Position: %{x}<br>Token: %{y}<br>Probability: %{z:.3f}<extra></extra>'\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(heatmap, row=i+1, col=1)\n",
    "    \n",
    "    # 更新布局\n",
    "    fig.update_layout(\n",
    "        title=\"Interactive Logits Heatmap Comparison\",\n",
    "        height=300 * n_models,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # 更新y轴标签\n",
    "    for i in range(n_models):\n",
    "        fig.update_yaxes(title_text=\"Target Tokens\", row=i+1, col=1)\n",
    "        fig.update_xaxes(title_text=\"Token Position\", row=i+1, col=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_difference_heatmap(results_dict: Dict[str, Dict]) -> None:\n",
    "    \"\"\"创建模型间差异热力图\"\"\"\n",
    "    \n",
    "    models = list(results_dict.keys())\n",
    "    n_comparisons = len(models) * (len(models) - 1) // 2\n",
    "    \n",
    "    if n_comparisons == 0:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_comparisons, figsize=(6 * n_comparisons, 5))\n",
    "    \n",
    "    if n_comparisons == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    comparison_idx = 0\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        for j in range(i + 1, len(models)):\n",
    "            model1, model2 = models[i], models[j]\n",
    "            \n",
    "            # 计算概率差异\n",
    "            prob1 = results_dict[model1]['summary_stats']['average_probabilities']\n",
    "            prob2 = results_dict[model2]['summary_stats']['average_probabilities']\n",
    "            \n",
    "            differences = []\n",
    "            for token in TARGET_TOKENS:\n",
    "                diff = prob2[token] - prob1[token]\n",
    "                differences.append(diff)\n",
    "            \n",
    "            # 创建差异矩阵（1D转2D用于可视化）\n",
    "            diff_matrix = np.array(differences).reshape(-1, 1)\n",
    "            \n",
    "            # 绘制热力图\n",
    "            im = axes[comparison_idx].imshow(diff_matrix, cmap='RdBu_r', \n",
    "                                           aspect='auto', vmin=-0.1, vmax=0.1)\n",
    "            \n",
    "            # 设置标签\n",
    "            axes[comparison_idx].set_title(f'{model2.upper()} - {model1.upper()}')\n",
    "            axes[comparison_idx].set_yticks(range(len(TARGET_TOKENS)))\n",
    "            axes[comparison_idx].set_yticklabels(TARGET_TOKENS)\n",
    "            axes[comparison_idx].set_xticks([])\n",
    "            \n",
    "            # 添加数值标注\n",
    "            for k, diff in enumerate(differences):\n",
    "                color = 'white' if abs(diff) > 0.05 else 'black'\n",
    "                axes[comparison_idx].text(0, k, f'{diff:.3f}', \n",
    "                                        ha='center', va='center', color=color, fontweight='bold')\n",
    "            \n",
    "            # 添加颜色条\n",
    "            cbar = plt.colorbar(im, ax=axes[comparison_idx], shrink=0.6)\n",
    "            cbar.set_label('Probability Difference', rotation=270, labelpad=15)\n",
    "            \n",
    "            comparison_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 演示热力图功能\n",
    "print(\"=== 创建高级可视化热力图 ===\")\n",
    "\n",
    "# 使用示例数据创建热力图\n",
    "sample_text = df.iloc[0]['full_text']\n",
    "demo_tokens = demo_tokenize(sample_text)\n",
    "demo_positions = {}\n",
    "for token in TARGET_TOKENS:\n",
    "    demo_positions[token] = [i for i, t in enumerate(demo_tokens) if t == token]\n",
    "\n",
    "# 创建模拟logits数据\n",
    "mock_logits_data = {\n",
    "    'baseline': np.random.randn(len(demo_tokens), 1000),\n",
    "    'sft': np.random.randn(len(demo_tokens), 1000),\n",
    "    'rl': np.random.randn(len(demo_tokens), 1000)\n",
    "}\n",
    "\n",
    "# 生成高级热力图\n",
    "create_advanced_heatmap(mock_logits_data, demo_tokens, demo_positions, \n",
    "                       \"Advanced Logits Analysis Heatmap\")\n",
    "\n",
    "# 创建交互式热力图\n",
    "print(\"\\\\n创建交互式热力图...\")\n",
    "interactive_fig = create_interactive_heatmap(mock_results, TARGET_TOKENS)\n",
    "interactive_fig.show()\n",
    "\n",
    "# 创建差异热力图\n",
    "print(\"\\\\n创建模型差异热力图...\")\n",
    "create_difference_heatmap(mock_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7a24d",
   "metadata": {},
   "source": [
    "# 11. Logits变化折线图\n",
    "\n",
    "展示关键词前后logits变化的动态模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logits_trends(model_results: Dict[str, Dict], target_tokens: List[str], \n",
    "                      context_window: int = 10) -> None:\n",
    "    \"\"\"绘制目标token周围的logits变化趋势\"\"\"\n",
    "    \n",
    "    n_tokens = len(target_tokens)\n",
    "    n_models = len(model_results)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_tokens, n_models, figsize=(5 * n_models, 4 * n_tokens))\n",
    "    \n",
    "    if n_tokens == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    if n_models == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for token_idx, target_token in enumerate(target_tokens):\n",
    "        for model_idx, (model_name, results) in enumerate(model_results.items()):\n",
    "            \n",
    "            # 模拟该token在不同位置的logits变化\n",
    "            sequence_positions = range(-context_window, context_window + 1)\n",
    "            \n",
    "            # 生成多条轨迹（代表不同的文本样本）\n",
    "            n_samples = 5\n",
    "            for sample_idx in range(n_samples):\n",
    "                # 模拟logits变化模式\n",
    "                base_logits = np.random.randn(len(sequence_positions)) * 0.5\n",
    "                \n",
    "                # 在目标位置(0)附近增加特定模式\n",
    "                peak_position = 0  # 目标token位置\n",
    "                for i, pos in enumerate(sequence_positions):\n",
    "                    if pos == peak_position:\n",
    "                        # 目标token位置的logits增强\n",
    "                        base_logits[i] += np.random.uniform(1, 3)\n",
    "                    elif abs(pos - peak_position) <= 2:\n",
    "                        # 目标token附近的logits变化\n",
    "                        distance_factor = 1 - abs(pos - peak_position) / 3\n",
    "                        base_logits[i] += np.random.uniform(0, 1) * distance_factor\n",
    "                \n",
    "                # 添加模型特定的模式\n",
    "                if model_name == 'sft' and target_token in ['think', 'check']:\n",
    "                    base_logits += 0.3  # SFT模型在推理token上更强\n",
    "                elif model_name == 'rl' and target_token in ['so', 'therefore']:\n",
    "                    base_logits += 0.4  # RL模型在结论token上更强\n",
    "                \n",
    "                # 绘制轨迹\n",
    "                alpha = 0.7 if sample_idx == 0 else 0.3\n",
    "                linewidth = 2 if sample_idx == 0 else 1\n",
    "                label = f'{model_name}_{target_token}' if sample_idx == 0 else None\n",
    "                \n",
    "                axes[token_idx, model_idx].plot(sequence_positions, base_logits, \n",
    "                                               color=colors[sample_idx], alpha=alpha, \n",
    "                                               linewidth=linewidth, label=label)\n",
    "            \n",
    "            # 标记目标token位置\n",
    "            axes[token_idx, model_idx].axvline(x=0, color='red', linestyle='--', \n",
    "                                             linewidth=2, alpha=0.8)\n",
    "            axes[token_idx, model_idx].axhline(y=0, color='gray', linestyle='-', \n",
    "                                             linewidth=0.5, alpha=0.5)\n",
    "            \n",
    "            # 设置标签和标题\n",
    "            axes[token_idx, model_idx].set_title(f'{target_token} - {model_name.upper()}')\n",
    "            axes[token_idx, model_idx].set_xlabel('Relative Position to Target Token')\n",
    "            axes[token_idx, model_idx].set_ylabel('Logits Value')\n",
    "            axes[token_idx, model_idx].grid(True, alpha=0.3)\n",
    "            \n",
    "            # 添加阴影区域表示目标token附近\n",
    "            axes[token_idx, model_idx].axvspan(-2, 2, alpha=0.1, color='yellow', \n",
    "                                             label='Target Context' if token_idx == 0 and model_idx == 0 else \"\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_interactive_trend_plot(model_results: Dict[str, Dict], target_tokens: List[str]) -> go.Figure:\n",
    "    \"\"\"创建交互式logits趋势图\"\"\"\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    context_window = 10\n",
    "    sequence_positions = list(range(-context_window, context_window + 1))\n",
    "    \n",
    "    colors = {'baseline': 'blue', 'sft': 'red', 'rl': 'green'}\n",
    "    \n",
    "    for model_name, results in model_results.items():\n",
    "        for target_token in target_tokens:\n",
    "            # 模拟logits变化\n",
    "            base_trend = np.random.randn(len(sequence_positions)) * 0.3\n",
    "            \n",
    "            # 在目标位置添加峰值\n",
    "            peak_idx = len(sequence_positions) // 2  # 中心位置\n",
    "            base_trend[peak_idx] += np.random.uniform(1, 2)\n",
    "            \n",
    "            # 添加平滑的上下文效应\n",
    "            for i in range(len(base_trend)):\n",
    "                distance_from_peak = abs(i - peak_idx)\n",
    "                if distance_from_peak <= 3:\n",
    "                    base_trend[i] += (3 - distance_from_peak) * 0.2\n",
    "            \n",
    "            # 添加模型特定效应\n",
    "            if model_name == 'sft':\n",
    "                base_trend += 0.2\n",
    "            elif model_name == 'rl':\n",
    "                base_trend += 0.3\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=sequence_positions,\n",
    "                y=base_trend,\n",
    "                mode='lines+markers',\n",
    "                name=f'{model_name}_{target_token}',\n",
    "                line=dict(color=colors.get(model_name, 'black'), width=2),\n",
    "                marker=dict(size=4),\n",
    "                hovertemplate=f'Model: {model_name}<br>Token: {target_token}<br>Position: %{{x}}<br>Logits: %{{y:.3f}}<extra></extra>'\n",
    "            ))\n",
    "    \n",
    "    # 添加目标位置标记\n",
    "    fig.add_vline(x=0, line_dash=\"dash\", line_color=\"red\", \n",
    "                  annotation_text=\"Target Token Position\")\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Interactive Logits Trends Around Target Tokens\",\n",
    "        xaxis_title=\"Relative Position to Target Token\",\n",
    "        yaxis_title=\"Logits Value\",\n",
    "        hovermode='x unified',\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def analyze_logits_patterns(model_results: Dict[str, Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"分析logits变化模式\"\"\"\n",
    "    \n",
    "    pattern_analysis = {}\n",
    "    \n",
    "    for model_name, results in model_results.items():\n",
    "        model_patterns = {}\n",
    "        \n",
    "        # 分析每个target token的模式\n",
    "        for token in TARGET_TOKENS:\n",
    "            avg_prob = results['summary_stats']['average_probabilities'][token]\n",
    "            std_prob = results['summary_stats']['std_probabilities'][token]\n",
    "            frequency = results['summary_stats']['token_frequencies'][token]\n",
    "            \n",
    "            # 计算模式指标\n",
    "            confidence_score = avg_prob / (std_prob + 1e-6)  # 避免除零\n",
    "            prominence_score = avg_prob * frequency  # 概率 × 频率\n",
    "            \n",
    "            model_patterns[token] = {\n",
    "                'confidence': confidence_score,\n",
    "                'prominence': prominence_score,\n",
    "                'variability': std_prob / (avg_prob + 1e-6),\n",
    "                'frequency_normalized': frequency / sum(results['summary_stats']['token_frequencies'].values())\n",
    "            }\n",
    "        \n",
    "        pattern_analysis[model_name] = model_patterns\n",
    "    \n",
    "    return pattern_analysis\n",
    "\n",
    "# 执行logits趋势分析\n",
    "print(\"=== Logits变化趋势分析 ===\")\n",
    "\n",
    "# 绘制趋势图\n",
    "plot_logits_trends(mock_results, TARGET_TOKENS[:4])  # 只显示前4个token避免过于拥挤\n",
    "\n",
    "# 创建交互式趋势图\n",
    "print(\"\\\\n创建交互式趋势图...\")\n",
    "interactive_trend_fig = create_interactive_trend_plot(mock_results, TARGET_TOKENS[:3])\n",
    "interactive_trend_fig.show()\n",
    "\n",
    "# 分析logits模式\n",
    "pattern_analysis = analyze_logits_patterns(mock_results)\n",
    "\n",
    "print(\"\\\\n=== Logits模式分析结果 ===\")\n",
    "for model_name, patterns in pattern_analysis.items():\n",
    "    print(f\"\\\\n{model_name.upper()} 模型模式:\")\n",
    "    \n",
    "    # 按confidence排序\n",
    "    sorted_patterns = sorted(patterns.items(), key=lambda x: x[1]['confidence'], reverse=True)\n",
    "    \n",
    "    print(\"  Top 3 最稳定的tokens (高confidence):\")\n",
    "    for token, metrics in sorted_patterns[:3]:\n",
    "        print(f\"    {token}: confidence={metrics['confidence']:.2f}, prominence={metrics['prominence']:.3f}\")\n",
    "    \n",
    "    # 按prominence排序\n",
    "    sorted_by_prominence = sorted(patterns.items(), key=lambda x: x[1]['prominence'], reverse=True)\n",
    "    print(\"  Top 3 最突出的tokens (高prominence):\")\n",
    "    for token, metrics in sorted_by_prominence[:3]:\n",
    "        print(f\"    {token}: prominence={metrics['prominence']:.3f}, variability={metrics['variability']:.2f}\")\n",
    "\n",
    "# 创建模式对比图\n",
    "def plot_pattern_comparison(pattern_analysis: Dict[str, Dict]) -> None:\n",
    "    \"\"\"绘制模式对比图\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 准备数据\n",
    "    models = list(pattern_analysis.keys())\n",
    "    tokens = TARGET_TOKENS\n",
    "    \n",
    "    metrics = ['confidence', 'prominence', 'variability', 'frequency_normalized']\n",
    "    metric_titles = ['Confidence Score', 'Prominence Score', 'Variability', 'Normalized Frequency']\n",
    "    \n",
    "    for i, (metric, title) in enumerate(zip(metrics, metric_titles)):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        \n",
    "        # 为每个模型创建数据\n",
    "        for j, model in enumerate(models):\n",
    "            values = [pattern_analysis[model][token][metric] for token in tokens]\n",
    "            x_positions = np.arange(len(tokens)) + j * 0.25\n",
    "            ax.bar(x_positions, values, width=0.25, label=model, alpha=0.8)\n",
    "        \n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Tokens')\n",
    "        ax.set_ylabel(title)\n",
    "        ax.set_xticks(np.arange(len(tokens)) + 0.25)\n",
    "        ax.set_xticklabels(tokens, rotation=45)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\\\n绘制模式对比图...\")\n",
    "plot_pattern_comparison(pattern_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd264c",
   "metadata": {},
   "source": [
    "# 12. 统计分析与模式发现\n",
    "\n",
    "进行统计测试，量化不同条件下的logits差异显著性，总结发现的模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def comprehensive_statistical_analysis(mock_results: Dict, level_analysis: Dict, \n",
    "                                     correctness_analysis: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"综合统计分析\"\"\"\n",
    "    \n",
    "    analysis_results = {\n",
    "        'model_comparisons': {},\n",
    "        'level_correlations': {},\n",
    "        'correctness_effects': {},\n",
    "        'principal_components': {},\n",
    "        'clustering_results': {}\n",
    "    }\n",
    "    \n",
    "    # 1. 模型间显著性检验\n",
    "    print(\"=== 1. 模型间差异显著性检验 ===\")\n",
    "    models = list(mock_results.keys())\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        for j in range(i + 1, len(models)):\n",
    "            model1, model2 = models[i], models[j]\n",
    "            \n",
    "            # 提取概率数据\n",
    "            probs1 = list(mock_results[model1]['summary_stats']['average_probabilities'].values())\n",
    "            probs2 = list(mock_results[model2]['summary_stats']['average_probabilities'].values())\n",
    "            \n",
    "            # t检验\n",
    "            t_stat, p_value = stats.ttest_rel(probs1, probs2)\n",
    "            \n",
    "            # Wilcoxon符号秩检验（非参数）\n",
    "            w_stat, w_p_value = stats.wilcoxon(probs1, probs2)\n",
    "            \n",
    "            comparison_key = f\"{model1}_vs_{model2}\"\n",
    "            analysis_results['model_comparisons'][comparison_key] = {\n",
    "                't_test': {'statistic': t_stat, 'p_value': p_value},\n",
    "                'wilcoxon': {'statistic': w_stat, 'p_value': w_p_value},\n",
    "                'effect_size': (np.mean(probs2) - np.mean(probs1)) / np.std(probs1 + probs2),\n",
    "                'significant': p_value < 0.05\n",
    "            }\n",
    "            \n",
    "            print(f\"{model1} vs {model2}:\")\n",
    "            print(f\"  t-test: t={t_stat:.3f}, p={p_value:.3f}\")\n",
    "            print(f\"  Wilcoxon: W={w_stat:.3f}, p={w_p_value:.3f}\")\n",
    "            print(f\"  Effect size: {analysis_results['model_comparisons'][comparison_key]['effect_size']:.3f}\")\n",
    "            print(f\"  Significant: {'Yes' if p_value < 0.05 else 'No'}\\\\n\")\n",
    "    \n",
    "    # 2. 难度级别相关性分析\n",
    "    print(\"=== 2. 难度级别相关性分析 ===\")\n",
    "    levels = sorted(level_analysis.keys())\n",
    "    \n",
    "    for token in TARGET_TOKENS:\n",
    "        frequencies = []\n",
    "        probabilities = []\n",
    "        \n",
    "        for level in levels:\n",
    "            if token in level_analysis[level]:\n",
    "                frequencies.append(level_analysis[level][token]['frequency'])\n",
    "                probabilities.append(level_analysis[level][token]['avg_probability'])\n",
    "            else:\n",
    "                frequencies.append(0)\n",
    "                probabilities.append(0)\n",
    "        \n",
    "        # Pearson相关性\n",
    "        freq_corr, freq_p = stats.pearsonr(levels, frequencies)\n",
    "        prob_corr, prob_p = stats.pearsonr(levels, probabilities)\n",
    "        \n",
    "        # Spearman相关性（非参数）\n",
    "        freq_spearman, freq_sp = stats.spearmanr(levels, frequencies)\n",
    "        prob_spearman, prob_sp = stats.spearmanr(levels, probabilities)\n",
    "        \n",
    "        analysis_results['level_correlations'][token] = {\n",
    "            'frequency_pearson': {'correlation': freq_corr, 'p_value': freq_p},\n",
    "            'probability_pearson': {'correlation': prob_corr, 'p_value': prob_p},\n",
    "            'frequency_spearman': {'correlation': freq_spearman, 'p_value': freq_sp},\n",
    "            'probability_spearman': {'correlation': prob_spearman, 'p_value': prob_sp}\n",
    "        }\n",
    "        \n",
    "        print(f\"{token}:\")\n",
    "        print(f\"  频率-难度相关性: r={freq_corr:.3f} (p={freq_p:.3f})\")\n",
    "        print(f\"  概率-难度相关性: r={prob_corr:.3f} (p={prob_p:.3f})\")\n",
    "    \n",
    "    # 3. 正确性效应分析\n",
    "    print(\"\\\\n=== 3. 正确性效应分析 ===\")\n",
    "    correct_data = correctness_analysis[True]\n",
    "    incorrect_data = correctness_analysis[False]\n",
    "    \n",
    "    for token in TARGET_TOKENS:\n",
    "        correct_freq = correct_data[token]['frequency']\n",
    "        incorrect_freq = incorrect_data[token]['frequency']\n",
    "        correct_prob = correct_data[token]['avg_probability']\n",
    "        incorrect_prob = incorrect_data[token]['avg_probability']\n",
    "        \n",
    "        # 卡方检验（频率）\n",
    "        obs_freq = np.array([[correct_freq * 100, (1-correct_freq) * 100],\n",
    "                            [incorrect_freq * 100, (1-incorrect_freq) * 100]])\n",
    "        chi2, chi2_p = stats.chi2_contingency(obs_freq)[:2]\n",
    "        \n",
    "        # t检验（概率）\n",
    "        # 模拟样本数据进行t检验\n",
    "        n_samples = 50\n",
    "        correct_samples = np.random.normal(correct_prob, correct_prob * 0.3, n_samples)\n",
    "        incorrect_samples = np.random.normal(incorrect_prob, incorrect_prob * 0.3, n_samples)\n",
    "        t_stat, t_p = stats.ttest_ind(correct_samples, incorrect_samples)\n",
    "        \n",
    "        analysis_results['correctness_effects'][token] = {\n",
    "            'frequency_chi2': {'statistic': chi2, 'p_value': chi2_p},\n",
    "            'probability_ttest': {'statistic': t_stat, 'p_value': t_p},\n",
    "            'frequency_effect_size': abs(correct_freq - incorrect_freq),\n",
    "            'probability_effect_size': abs(correct_prob - incorrect_prob)\n",
    "        }\n",
    "        \n",
    "        print(f\"{token}:\")\n",
    "        print(f\"  频率差异: χ²={chi2:.3f} (p={chi2_p:.3f})\")\n",
    "        print(f\"  概率差异: t={t_stat:.3f} (p={t_p:.3f})\")\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "def dimensionality_reduction_analysis(mock_results: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"降维分析\"\"\"\n",
    "    \n",
    "    print(\"\\\\n=== 4. 主成分分析 (PCA) ===\")\n",
    "    \n",
    "    # 准备数据矩阵\n",
    "    data_matrix = []\n",
    "    labels = []\n",
    "    \n",
    "    for model_name, results in mock_results.items():\n",
    "        prob_vector = list(results['summary_stats']['average_probabilities'].values())\n",
    "        data_matrix.append(prob_vector)\n",
    "        labels.append(model_name)\n",
    "    \n",
    "    data_matrix = np.array(data_matrix)\n",
    "    \n",
    "    # 执行PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(data_matrix)\n",
    "    \n",
    "    print(f\"主成分解释的方差比例: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"累积解释方差: {np.sum(pca.explained_variance_ratio_):.3f}\")\n",
    "    \n",
    "    # 绘制PCA结果\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    for i, (model, color) in enumerate(zip(labels, colors)):\n",
    "        plt.scatter(pca_result[i, 0], pca_result[i, 1], c=color, s=100, label=model, alpha=0.8)\n",
    "        plt.annotate(model, (pca_result[i, 0], pca_result[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\\n    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\\n    plt.title('PCA: Model Comparison in Token Probability Space')\\n    plt.legend()\\n    plt.grid(True, alpha=0.3)\\n    \\n    # 特征重要性\\n    plt.subplot(1, 2, 2)\\n    feature_importance = np.abs(pca.components_[0])  # 第一主成分\\n    indices = np.argsort(feature_importance)[::-1]\\n    \\n    plt.bar(range(len(TARGET_TOKENS)), feature_importance[indices])\\n    plt.xlabel('Token Features')\\n    plt.ylabel('PC1 Loading (Absolute Value)')\\n    plt.title('Feature Importance in First Principal Component')\\n    plt.xticks(range(len(TARGET_TOKENS)), [TARGET_TOKENS[i] for i in indices], rotation=45)\\n    plt.grid(True, alpha=0.3)\\n    \\n    plt.tight_layout()\\n    plt.show()\\n    \\n    return {\\n        'pca_result': pca_result,\\n        'explained_variance_ratio': pca.explained_variance_ratio_,\\n        'components': pca.components_,\\n        'feature_importance': feature_importance\\n    }\\n\\ndef clustering_analysis(mock_results: Dict) -> Dict[str, Any]:\\n    \\\"\\\"\\\"聚类分析\\\"\\\"\\\"\\n    \\n    print(\\\"\\\\n=== 5. 聚类分析 ===\\\\n\\\")\\n    \\n    # 准备数据：将所有token概率作为特征\\n    all_data = []\\n    all_labels = []\\n    \\n    for model_name, results in mock_results.items():\\n        probs = list(results['summary_stats']['average_probabilities'].values())\\n        stds = list(results['summary_stats']['std_probabilities'].values())\\n        freqs = list(results['summary_stats']['token_frequencies'].values())\\n        \\n        # 组合特征：概率 + 标准差 + 频率\\n        combined_features = probs + stds + freqs\\n        all_data.append(combined_features)\\n        all_labels.append(model_name)\\n    \\n    all_data = np.array(all_data)\\n    \\n    # 标准化数据\\n    from sklearn.preprocessing import StandardScaler\\n    scaler = StandardScaler()\\n    scaled_data = scaler.fit_transform(all_data)\\n    \\n    # K-means聚类\\n    kmeans = KMeans(n_clusters=2, random_state=42)\\n    cluster_labels = kmeans.fit_predict(scaled_data)\\n    \\n    print(\\\"聚类结果:\\\")\\n    for i, (model, cluster) in enumerate(zip(all_labels, cluster_labels)):\\n        print(f\\\"  {model}: Cluster {cluster}\\\")\\n    \\n    # 计算聚类质量指标\\n    from sklearn.metrics import silhouette_score\\n    silhouette_avg = silhouette_score(scaled_data, cluster_labels)\\n    print(f\\\"\\\\n轮廓系数 (Silhouette Score): {silhouette_avg:.3f}\\\")\\n    \\n    return {\\n        'cluster_labels': cluster_labels,\\n        'silhouette_score': silhouette_avg,\\n        'cluster_centers': kmeans.cluster_centers_\\n    }\\n\\ndef generate_research_summary(analysis_results: Dict, pattern_analysis: Dict) -> None:\\n    \\\"\\\"\\\"生成研究总结\\\"\\\"\\\"\\n    \\n    print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n    print(\\\"                  研究发现总结\\\")\\n    print(\\\"=\\\"*60)\\n    \\n    # 1. 模型间主要差异\\n    print(\\\"\\\\n1. 模型间主要差异:\\\")\\n    significant_comparisons = []\\n    for comparison, results in analysis_results['model_comparisons'].items():\\n        if results['significant']:\\n            significant_comparisons.append(comparison)\\n            effect_size = results['effect_size']\\n            direction = \\\"增强\\\" if effect_size > 0 else \\\"减弱\\\"\\n            print(f\\\"   • {comparison.replace('_vs_', ' → ')}: {direction} (效应量: {abs(effect_size):.3f})\\\")\\n    \\n    if not significant_comparisons:\\n        print(\\\"   • 未发现模型间显著差异\\\")\\n    \\n    # 2. 关键发现\\n    print(\\\"\\\\n2. 关键发现:\\\")\\n    \\n    # 最稳定的tokens\\n    all_confidence = {}\\n    for model, patterns in pattern_analysis.items():\\n        for token, metrics in patterns.items():\\n            if token not in all_confidence:\\n                all_confidence[token] = []\\n            all_confidence[token].append(metrics['confidence'])\\n    \\n    avg_confidence = {token: np.mean(confs) for token, confs in all_confidence.items()}\\n    top_stable_tokens = sorted(avg_confidence.items(), key=lambda x: x[1], reverse=True)[:3]\\n    \\n    print(\\\"   • 最稳定的推理词汇:\\\")\\n    for token, conf in top_stable_tokens:\\n        print(f\\\"     - {token} (平均confidence: {conf:.2f})\\\")\\n    \\n    # 难度级别效应\\n    print(\\\"\\\\n   • 难度级别效应:\\\")\\n    strong_correlations = []\\n    for token, corrs in analysis_results['level_correlations'].items():\\n        freq_corr = corrs['frequency_pearson']['correlation']\\n        if abs(freq_corr) > 0.5 and corrs['frequency_pearson']['p_value'] < 0.05:\\n            direction = \\\"正相关\\\" if freq_corr > 0 else \\\"负相关\\\"\\n            strong_correlations.append(f\\\"{token} ({direction}, r={freq_corr:.2f})\\\")\\n    \\n    if strong_correlations:\\n        for corr in strong_correlations:\\n            print(f\\\"     - {corr}\\\")\\n    else:\\n        print(\\\"     - 未发现显著的难度级别效应\\\")\\n    \\n    # 正确性效应\\n    print(\\\"\\\\n   • 正确性效应:\\\")\\n    significant_correctness = []\\n    for token, effects in analysis_results['correctness_effects'].items():\\n        if effects['probability_ttest']['p_value'] < 0.05:\\n            effect_size = effects['probability_effect_size']\\n            significant_correctness.append(f\\\"{token} (效应量: {effect_size:.3f})\\\")\\n    \\n    if significant_correctness:\\n        for effect in significant_correctness:\\n            print(f\\\"     - {effect}\\\")\\n    else:\\n        print(\\\"     - 未发现显著的正确性效应\\\")\\n    \\n    # 3. 实践建议\\n    print(\\\"\\\\n3. 实践建议:\\\")\\n    print(\\\"   • 模型改进方向:\\\")\\n    if len(significant_comparisons) > 0:\\n        print(\\\"     - 重点关注显著差异的token，分析训练策略\\\")\\n        print(\\\"     - 考虑增强表现较弱模型的推理能力\\\")\\n    \\n    print(\\\"   • 进一步研究:\\\")\\n    print(\\\"     - 增加更多样本以提高统计功效\\\")\\n    print(\\\"     - 分析更多上下文窗口大小的影响\\\")\\n    print(\\\"     - 研究token组合的交互效应\\\")\\n    \\n    print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\\n# 执行综合分析\\nprint(\\\"开始执行综合统计分析...\\\")\\n\\n# 统计分析\\nanalysis_results = comprehensive_statistical_analysis(mock_results, level_analysis, correctness_analysis)\\n\\n# 降维分析\\npca_results = dimensionality_reduction_analysis(mock_results)\\n\\n# 聚类分析\\nclustering_results = clustering_analysis(mock_results)\\n\\n# 生成研究总结\\ngenerate_research_summary(analysis_results, pattern_analysis)\\n\\n# 保存结果\\nprint(\\\"\\\\n保存分析结果...\\\")\\nfinal_results = {\\n    'statistical_analysis': analysis_results,\\n    'pca_analysis': {\\n        'explained_variance_ratio': pca_results['explained_variance_ratio'].tolist(),\\n        'feature_importance': pca_results['feature_importance'].tolist()\\n    },\\n    'clustering_analysis': {\\n        'silhouette_score': clustering_results['silhouette_score'],\\n        'cluster_labels': clustering_results['cluster_labels'].tolist()\\n    },\\n    'pattern_analysis': pattern_analysis\\n}\\n\\n# 可以保存到JSON文件\\nimport json\\nwith open('results/comprehensive_analysis_results.json', 'w', encoding='utf-8') as f:\\n    json.dump(final_results, f, ensure_ascii=False, indent=2)\\n\\nprint(\\\"分析完成！结果已保存至 results/comprehensive_analysis_results.json\\\")\\nprint(\\\"\\\\n=== Notebook演示完成 ===\\\")\\nprint(\\\"在实际应用中，请将模拟数据替换为真实的模型logits数据。\\\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
